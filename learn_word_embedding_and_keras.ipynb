{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isfocus</th>\n",
       "      <th>focus</th>\n",
       "      <th>word</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>index</th>\n",
       "      <th>POS</th>\n",
       "      <th>-1POS</th>\n",
       "      <th>-2POS</th>\n",
       "      <th>+1POS</th>\n",
       "      <th>+2POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>chapter</td>\n",
       "      <td>Chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>CD</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>.</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>chapter</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>CD</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>see</td>\n",
       "      <td>Once</td>\n",
       "      <td>once</td>\n",
       "      <td>0</td>\n",
       "      <td>RB</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>WRB</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>see</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>1</td>\n",
       "      <td>WRB</td>\n",
       "      <td>RB</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>PRP</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isfocus    focus     word normalized_words  index  POS -1POS -2POS +1POS  \\\n",
       "0        1  chapter  Chapter          chapter      0   NN  XXXX  XXXX    CD   \n",
       "1        0  chapter        1                1      1   CD    NN  XXXX     .   \n",
       "2        0  chapter        .                .      2    .    CD    NN  XXXX   \n",
       "3        0      see     Once             once      0   RB  XXXX  XXXX   WRB   \n",
       "4        0      see     when             when      1  WRB    RB  XXXX   PRP   \n",
       "\n",
       "  +2POS  \n",
       "0     .  \n",
       "1  XXXX  \n",
       "2  XXXX  \n",
       "3   PRP  \n",
       "4   VBD  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('data/amr-bank-struct-v1.6.csv', encoding = 'utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>isfocus</th>\n",
       "      <th>index</th>\n",
       "      <th>POS</th>\n",
       "      <th>-1POS</th>\n",
       "      <th>-2POS</th>\n",
       "      <th>+1POS</th>\n",
       "      <th>+2POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>CD</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>.</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>CD</td>\n",
       "      <td>NN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once</td>\n",
       "      <td>once</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RB</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>WRB</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>WRB</td>\n",
       "      <td>RB</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>PRP</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word normalized_words  isfocus  index  POS -1POS -2POS +1POS +2POS\n",
       "0  Chapter          chapter        1      0   NN  XXXX  XXXX    CD     .\n",
       "1        1                1        0      1   CD    NN  XXXX     .  XXXX\n",
       "2        .                .        0      2    .    CD    NN  XXXX  XXXX\n",
       "3     Once             once        0      0   RB  XXXX  XXXX   WRB   PRP\n",
       "4     when             when        0      1  WRB    RB  XXXX   PRP   VBD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.loc[:,['word','normalized_words','isfocus','index','POS','-1POS','-2POS','+1POS','+2POS']]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['normalized_words'] = x['normalized_words']+' '+x['index'].astype(str)+' '+x['POS']+' '+x['-1POS']+' '+x['-1POS']+' '+x['-2POS']+' '+x['+1POS']+' '+x['+2POS']\n",
    "x['normalized_words'] = x['word']+' '+x['normalized_words']+' '+x['index'].astype(str)+' '+x['POS']+' '+x['-1POS']+' '+x['-1POS']+' '+x['-2POS']+' '+x['+1POS']+' '+x['+2POS']\n",
    "x = x.iloc[0:20000,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_0 = x[df.isfocus == 0]\n",
    "# df_1 = x[df.isfocus == 1]\n",
    "\n",
    "# df_majority_downsampled = resample(df_0, \n",
    "#                                  replace=False,\n",
    "#                                  n_samples=df_1.size,     \n",
    "#                                  random_state=123) \n",
    "\n",
    "# df_downsampled = pd.concat([df_majority_downsampled, df_1])\n",
    "\n",
    "# X = df_downsampled['normalized_words'].values\n",
    "# y = df_downsampled['isfocus'].values\n",
    "X = x['normalized_words'].values\n",
    "y = x['isfocus'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5,\n",
    "                                                    random_state = 1234,stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer_obj = Tokenizer()\n",
    "total_words = X_train + X_test\n",
    "tokenizer_obj.fit_on_texts(total_words)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_words])\n",
    "\n",
    "vocab_size =len(tokenizer_obj.word_index)+1\n",
    "\n",
    "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen = max_length, padding = 'post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen = max_length, padding ='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 17, 100)           558700    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 571,501\n",
      "Trainable params: 571,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM =100\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length))\n",
    "model.add(GRU(units=32, dropout =0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.3159 - acc: 0.9300 - val_loss: 0.1999 - val_acc: 0.9492\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2043 - acc: 0.9493 - val_loss: 0.1991 - val_acc: 0.9492\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9493 - val_loss: 0.1905 - val_acc: 0.9492\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.1656 - acc: 0.9503 - val_loss: 0.1342 - val_acc: 0.9564\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.1251 - acc: 0.9574 - val_loss: 0.1181 - val_acc: 0.9638\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.1011 - acc: 0.9616 - val_loss: 0.1234 - val_acc: 0.9623\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.0936 - acc: 0.9661 - val_loss: 0.1253 - val_acc: 0.9607\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.0850 - acc: 0.9696 - val_loss: 0.1314 - val_acc: 0.9588\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.0782 - acc: 0.9711 - val_loss: 0.1355 - val_acc: 0.9567\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.0790 - acc: 0.9695 - val_loss: 0.1348 - val_acc: 0.9592\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.0757 - acc: 0.9727 - val_loss: 0.1366 - val_acc: 0.9586\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.0742 - acc: 0.9719 - val_loss: 0.1357 - val_acc: 0.9565\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.0704 - acc: 0.9732 - val_loss: 0.1438 - val_acc: 0.9603\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.0715 - acc: 0.9765 - val_loss: 0.1365 - val_acc: 0.9561\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.0676 - acc: 0.9751 - val_loss: 0.1402 - val_acc: 0.9584\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.0645 - acc: 0.9764 - val_loss: 0.1401 - val_acc: 0.9573\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.0659 - acc: 0.9751 - val_loss: 0.1401 - val_acc: 0.9581\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.0654 - acc: 0.9754 - val_loss: 0.1368 - val_acc: 0.9580\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.0640 - acc: 0.9763 - val_loss: 0.1403 - val_acc: 0.9539\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.0638 - acc: 0.9776 - val_loss: 0.1372 - val_acc: 0.9577\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.0593 - acc: 0.9781 - val_loss: 0.1423 - val_acc: 0.9587\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.0595 - acc: 0.9778 - val_loss: 0.1393 - val_acc: 0.9571\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.0573 - acc: 0.9791 - val_loss: 0.1399 - val_acc: 0.9560\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.0591 - acc: 0.9756 - val_loss: 0.1389 - val_acc: 0.9570\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.0585 - acc: 0.9769 - val_loss: 0.1406 - val_acc: 0.9586\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.0575 - acc: 0.9789 - val_loss: 0.1378 - val_acc: 0.9587\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.0551 - acc: 0.9779 - val_loss: 0.1437 - val_acc: 0.9592\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.0535 - acc: 0.9790 - val_loss: 0.1456 - val_acc: 0.9583\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.0557 - acc: 0.9773 - val_loss: 0.1437 - val_acc: 0.9578\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.0534 - acc: 0.9780 - val_loss: 0.1471 - val_acc: 0.9582\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.0544 - acc: 0.9776 - val_loss: 0.1459 - val_acc: 0.9579\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.0526 - acc: 0.9792 - val_loss: 0.1472 - val_acc: 0.9592\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.0520 - acc: 0.9795 - val_loss: 0.1461 - val_acc: 0.9596\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.0512 - acc: 0.9786 - val_loss: 0.1472 - val_acc: 0.9592\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.0506 - acc: 0.9795 - val_loss: 0.1519 - val_acc: 0.9590\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.0496 - acc: 0.9802 - val_loss: 0.1554 - val_acc: 0.9591\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.0500 - acc: 0.9795 - val_loss: 0.1491 - val_acc: 0.9595\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.0488 - acc: 0.9788 - val_loss: 0.1530 - val_acc: 0.9590\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.0475 - acc: 0.9803 - val_loss: 0.1571 - val_acc: 0.9592\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.0494 - acc: 0.9791 - val_loss: 0.1544 - val_acc: 0.9593\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.0474 - acc: 0.9812 - val_loss: 0.1597 - val_acc: 0.9586\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.0501 - acc: 0.9799 - val_loss: 0.1571 - val_acc: 0.9568\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.0452 - acc: 0.9810 - val_loss: 0.1595 - val_acc: 0.9589\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.0470 - acc: 0.9807 - val_loss: 0.1595 - val_acc: 0.9588\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.0467 - acc: 0.9796 - val_loss: 0.1596 - val_acc: 0.9580\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.0454 - acc: 0.9805 - val_loss: 0.1639 - val_acc: 0.9581\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.0443 - acc: 0.9810 - val_loss: 0.1668 - val_acc: 0.9586\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.0456 - acc: 0.9807 - val_loss: 0.1690 - val_acc: 0.9585\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.0462 - acc: 0.9808 - val_loss: 0.1675 - val_acc: 0.9581\n",
      "Epoch 50/50\n",
      " - 4s - loss: 0.0447 - acc: 0.9810 - val_loss: 0.1683 - val_acc: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24b7ef98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=128, epochs =50, validation_data= (X_test_pad, \n",
    "                                                                            y_test),verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x = X_test_pad)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] >= 0.9:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.43454038997214495\n",
      "precision_score: 0.7428571428571429\n",
      "recall_score: 0.30708661417322836\n",
      "[[9438   54]\n",
      " [ 352  156]]\n",
      "0.9594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "print ('f1_score: ',f1_score(y_true=y_test, y_pred=y_pred))\n",
    "print ('precision_score:',precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print ('recall_score:',recall_score(y_true=y_test, y_pred=y_pred))\n",
    "cm_svm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print (cm_svm)\n",
    "def accuracy(cm):\n",
    "    return np.trace(cm)/np.sum(cm)\n",
    "print (accuracy(cm_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00641237],\n",
       "       [0.0059375 ],\n",
       "       [0.00600799],\n",
       "       [0.0052671 ],\n",
       "       [0.00553304],\n",
       "       [0.00549879]], dtype=float32)"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "# test_sample_1 = 'say 9 VBD NN NN DT , ``'\n",
    "# test_sample_2 = 'prince 9 NN JJ JJ DT , CC'\n",
    "# test_sample_3 = 'reveal 26 VBN VBD VBD NN TO PRP'\n",
    "# test_sample_4 = 'chapter 0 NN XXXX XXXX XXXX CD .'\n",
    "# test_sample_5 = 'little 21 JJ DT DT IN NN POS'\n",
    "# test_sample_6 = 'i 1 PRP `` XXXX VBP RB'\n",
    "\n",
    "test_sample_1 = 'see 6 VBN VBN VBN RB IN DT'\n",
    "test_sample_2 = 'present 8 VBN VBD VBD NN PRP TO'\n",
    "test_sample_3 = 'account 24 NN IN IN VBZ IN DT'\n",
    "test_sample_4 = 'he 1 PRP CC CC XXXX VBD IN'\n",
    "test_sample_5 = 'in 3 IN VBD VBD PRP JJ NN'\n",
    "test_sample_6 = 'of 23 IN NN NN IN NN ,'\n",
    "test_samples=[test_sample_1,test_sample_2,test_sample_3,test_sample_4,test_sample_5,test_sample_6]\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences (test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
    "\n",
    "model.predict(x=test_samples_tokens_pad)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-299-16b4a43ecc59>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-299-16b4a43ecc59>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    0,contrast,in,in,3,IN,VBD,PRP,JJ,NN\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
